{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一个Demo，惊鸿一瞥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "首次编译运行时，HanLP会自动构建词典缓存，请稍候……\n",
      "[你好/vl, ，/w, 欢迎/v, 使用/v, HanLP/nx, 汉语/gi, 处理/vn, 包/v, ！/w, 接下来/vl, 请/v, 从/p, 其他/rzv, Demo/nx, 中/f, 体验/v, HanLP/nx, 丰富/a, 的/ude1, 功能/n, ~/nx]\n"
     ]
    }
   ],
   "source": [
    "# 第一个Demo，惊鸿一瞥\n",
    "print(\"首次编译运行时，HanLP会自动构建词典缓存，请稍候……\")\n",
    "# HanLP.Config.enableDebug()\n",
    "print(HanLP.segment(\n",
    "    \"你好，欢迎使用HanLP汉语处理包！接下来请从其他Demo中体验HanLP丰富的功能~\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示基础分词，基础分词只进行基本NGram分词，不识别命名实体，不使用用户词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[举办/v, 纪念活动/nz, 铭记/v, 二战/n, 历史/n, ，/w, 不忘/v, 战争/n, 带给/v, 人类/n, 的/ude1, 深重/a, 灾难/n, ，/w, 是/vshi, 为了/p, 防止/v, 悲剧/n, 重演/v, ，/w, 确保/v, 和平/n, 永驻/nz, ；/w, 铭记/v, 二战/n, 历史/n, ，/w, 更是/d, 为了/p, 提醒/v, 国际/n, 社会/n, ，/w, 需要/v, 共同/d, 捍卫/v, 二战/n, 胜利/vn, 成果/n, 和/cc, 国际/n, 公平/a, 正义/n, ，/w, 必须/d, 警惕/v, 和/cc, 抵制/v, 在/p, 历史/n, 认知/vn, 和/cc, 维护/v, 战后/t, 国际/n, 秩序/n, 问题/n, 上/f, 的/ude1, 倒行逆施/vl, 。/w]\n"
     ]
    }
   ],
   "source": [
    "# 演示基础分词，基础分词只进行基本NGram分词，不识别命名实体，不使用用户词典\n",
    "text = (\"举办纪念活动铭记二战历史，不忘战争带给人类的深重灾难，是为了防止悲剧重演，确保和平永驻；\"\n",
    "        \"铭记二战历史，更是为了提醒国际社会，需要共同捍卫二战胜利成果和国际公平正义，\"\n",
    "        \"必须警惕和抵制在历史认知和维护战后国际秩序问题上的倒行逆施。\")\n",
    "BasicTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.BasicTokenizer\")\n",
    "print(BasicTokenizer.segment(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示自动去除停用词、自动断句的分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
      "[小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz]\n",
      "[居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n"
     ]
    }
   ],
   "source": [
    "# 演示自动去除停用词、自动断句的分词器\n",
    "Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "NotionalTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NotionalTokenizer\")\n",
    "\n",
    "text = \"小区居民有的反对喂养流浪猫，而有的居民却赞成喂养这些小宝贝\"\n",
    "print(NotionalTokenizer.segment(text))\n",
    "for sentence in NotionalTokenizer.seg2sentence(text):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示动态设置预置分词器，这里的设置是全局的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[泽田依/nr, 子/ng, 是/vshi, 上外/nit, 日本/ns, 文化/n, 经济学院/nit, 的/ude1, 外教/n]\n",
      "[泽田依子/nrj, 是/vshi, 上外日本文化经济学院/nt, 的/ude1, 外教/n]\n"
     ]
    }
   ],
   "source": [
    "# 演示动态设置预置分词器，这里的设置是全局的\n",
    "text = \"泽田依子是上外日本文化经济学院的外教\"\n",
    "StandardTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.StandardTokenizer\")\n",
    "print(StandardTokenizer.segment(text))\n",
    "StandardTokenizer.SEGMENT.enableAllNamedEntityRecognize(True)\n",
    "print(StandardTokenizer.segment(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HanLP, 是, 由, 一系列, 模型, 与, 算法, 组成, 的, Java, 工具包, ，, 目标, 是, 普及, 自然, 语言, 处理, 在, 生产, 环境, 中, 的, 应用, 。]\n",
      "[鐵桿, 部隊, 憤怒, 情緒, 集結,  , 馬英九, 腹背受敵]\n",
      "[馬英九, 回應, 連勝文, “, 丐幫, 說, ”, ：, 稱, 黨內, 同志, 談話, 應, 謹慎]\n",
      "[高锰酸钾, ，, 强, 氧化剂, ，, 紫红色, 晶体, ，, 可, 溶于, 水, ，, 遇, 乙醇, 即, 被, 还原, 。, 常用, 作, 消毒剂, 、, 水, 净化剂, 、, 氧化剂, 、, 漂白剂, 、, 毒气, 吸收剂, 、, 二氧化碳, 精制剂, 等, 。]\n",
      "[《, 夜晚, 的, 骰子, 》, 通过, 描述, 浅草, 的, 舞女, 在, 暗夜, 中, 扔, 骰子, 的, 情景, ,, 寄托, 了, 作者, 对, 庶民, 生活区, 的, 情感]\n",
      "[这个, 像, 是, 真的, [, 委屈, ], 前面, 那个, 打扮, 太江, 户, 了, ，, 一点, 不, 上品, ., ., ., @, hankcs]\n",
      "[鼎泰丰, 的, 小笼, 一点, 味道, 也, 没有, ., ., ., 每样, 都, 淡淡的, ., ., ., 淡淡的, ，, 哪, 有, 食堂, 2, A, 的, 好次]\n",
      "[克里斯蒂娜·克罗尔, 说, ：, 不, ，, 我, 不是, 虎妈, 。, 我, 全家, 都, 热爱, 音乐, ，, 我, 也, 鼓励, 他们, 这么, 做, 。]\n",
      "[今日, APPS, ：, Sago Mini Toolbox, 培养, 孩子, 动手, 能力]\n",
      "[财政部, 副部长, 王保安, 调任, 国家, 统计局, 党组, 书记]\n",
      "[2.34, 米, 男子, 娶, 1.53, 米, 女, 粉丝,  , 称, 夫妻, 生活, 没问题]\n",
      "[你, 看过, 穆赫兰道, 吗]\n",
      "[国办, 发布, 网络, 提速, 降费, 十四条, 指导, 意见,  , 鼓励, 流量, 不, 清零]\n",
      "[乐, 视, 超级, 手机, 能否, 承载, 贾布斯, 的, 生态, 梦]\n"
     ]
    }
   ],
   "source": [
    "# CRF分词，下载data-for-1.6.4.zip\n",
    "sentence_array = [\n",
    "    \"HanLP是由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用。\",\n",
    "    \"鐵桿部隊憤怒情緒集結 馬英九腹背受敵\",  # 繁体无压力\n",
    "    \"馬英九回應連勝文“丐幫說”：稱黨內同志談話應謹慎\",\n",
    "    \"高锰酸钾，强氧化剂，紫红色晶体，可溶于水，遇乙醇即被还原。常用作消毒剂、水净化剂、氧化剂、漂白剂、毒气吸收剂、二氧化碳精制剂等。\",\n",
    "    \"《夜晚的骰子》通过描述浅草的舞女在暗夜中扔骰子的情景,寄托了作者对庶民生活区的情感\",\n",
    "    \"这个像是真的[委屈]前面那个打扮太江户了，一点不上品...@hankcs\",\n",
    "    \"鼎泰丰的小笼一点味道也没有...每样都淡淡的...淡淡的，哪有食堂2A的好次\",\n",
    "    \"克里斯蒂娜·克罗尔说：不，我不是虎妈。我全家都热爱音乐，我也鼓励他们这么做。\",\n",
    "    \"今日APPS：Sago Mini Toolbox培养孩子动手能力\",\n",
    "    \"财政部副部长王保安调任国家统计局党组书记\",\n",
    "    \"2.34米男子娶1.53米女粉丝 称夫妻生活没问题\",\n",
    "    \"你看过穆赫兰道吗\",\n",
    "    \"国办发布网络提速降费十四条指导意见 鼓励流量不清零\",\n",
    "    \"乐视超级手机能否承载贾布斯的生态梦\"\n",
    "]\n",
    "Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "Config.ShowTermNature = False\n",
    "CRFSegment = JClass(\"com.hankcs.hanlp.seg.CRF.CRFSegment\")\n",
    "segment = CRFSegment().enableCustomDictionary(False)\n",
    "\n",
    "for sentence in sentence_array:\n",
    "    term_list = segment.seg(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示极速分词，基于DoubleArrayTrie实现的词典正向最长分词，适用于“高吞吐量”“精度一般”的场合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[江西, 鄱阳湖, 干枯, ，, 中国, 最大, 淡水湖, 变成, 大草原]\n"
     ]
    }
   ],
   "source": [
    "# 演示极速分词，基于DoubleArrayTrie实现的词典正向最长分词，适用于“高吞吐量”“精度一般”的场合\n",
    "SpeedTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.SpeedTokenizer\")\n",
    "text = \"江西鄱阳湖干枯，中国最大淡水湖变成大草原\"\n",
    "JClass(\"com.hankcs.hanlp.HanLP$Config\").ShowTermNature = False\n",
    "print(SpeedTokenizer.segment(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示二阶隐马分词，这是一种基于字标注的分词方法，对未登录词支持较好，对已登录词的分词速度慢。综合性能不如CRF分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HanLP是, 由, 一系列, 模型, 与, 算法, 组成, 的, Java, 工具, 包, ，, 目标, 是, 普及, 自然语言处理, 在, 生产, 环境, 中的, 应用, 。]\n",
      "[高锰酸钾, ，, 强氧化剂, ，, 紫红色, 晶体, ，, 可, 溶于, 水, ，, 遇乙醇, 即, 被, 还原, 。, 常, 用作, 消毒, 剂, 、, 水, 净化剂, 、, 氧化剂, 、, 漂白剂, 、, 毒气, 吸收剂, 、, 二氧化碳, 精制剂, 等, 。]\n",
      "[《, 夜晚, 的, 骰子, 》, 通过, 描述, 浅草, 的, 舞女, 在, 暗夜, 中, 扔骰子, 的, 情景, ,, 寄托, 了, 作者, 对, 庶民, 生活区, 的, 情感]\n",
      "[这个, 像, 是, 真的, [, 委屈, ], 前面, 那个, 打扮, 太, 江, 户, 了, ，, 一点, 不, 上品, ..., @, hankcs]\n",
      "[鼎泰丰, 的, 小笼, 一点, 味道, 也, 没有, ..., 每样, 都, 淡淡, 的, ..., 淡淡的, ，, 哪, 有, 食堂, 2A, 的, 好次]\n",
      "[克里斯蒂娜·克罗尔, 说, ：, 不, ，, 我, 不是, 虎妈, 。, 我, 全家, 都, 热爱, 音乐, ，, 我, 也, 鼓励, 他们, 这么, 做, 。]\n",
      "[今日, APPS, ：, Sago , Mini , Toolbox, 培养, 孩子, 动手, 能力]\n",
      "[财政部, 副部长, 王保安, 调任, 国家, 统计局, 党组, 书记]\n",
      "[2, ., 34, 米, 男子, 娶, 1, ., 53, 米, 女, 粉丝,  , 称, 夫妻, 生活, 没问题]\n",
      "[你, 看过, 穆赫, 兰道, 吗]\n",
      "[乐, 视, 超级, 手机, 能否, 承载, 贾布斯, 的, 生态, 梦]\n"
     ]
    }
   ],
   "source": [
    "# 下载data-for-1.6.4.zip\n",
    "# 演示二阶隐马分词，这是一种基于字标注的分词方法，对未登录词支持较好，对已登录词的分词速度慢。综合性能不如CRF分词。\n",
    "# 还未稳定，请不要用于生产环境。二阶隐马标注分词效果尚且不好，许多开源分词器使用甚至使用一阶隐马（BiGram二元文法），\n",
    "# 效果可想而知。对基于字符的序列标注分词方法，hackcs只推荐CRF。\n",
    "HMMSegment = JClass(\"com.hankcs.hanlp.seg.HMM.HMMSegment\")\n",
    "Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "\n",
    "JClass(\"com.hankcs.hanlp.HanLP$Config\").ShowTermNature = False\n",
    "segment = HMMSegment()\n",
    "sentence_array = [\n",
    "    \"HanLP是由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用。\",\n",
    "    \"高锰酸钾，强氧化剂，紫红色晶体，可溶于水，遇乙醇即被还原。常用作消毒剂、水净化剂、氧化剂、漂白剂、毒气吸收剂、二氧化碳精制剂等。\",  # 专业名词有一定辨识能力\n",
    "    \"《夜晚的骰子》通过描述浅草的舞女在暗夜中扔骰子的情景,寄托了作者对庶民生活区的情感\",  # 非新闻语料\n",
    "    \"这个像是真的[委屈]前面那个打扮太江户了，一点不上品...@hankcs\",  # 微博\n",
    "    \"鼎泰丰的小笼一点味道也没有...每样都淡淡的...淡淡的，哪有食堂2A的好次\",\n",
    "    \"克里斯蒂娜·克罗尔说：不，我不是虎妈。我全家都热爱音乐，我也鼓励他们这么做。\",\n",
    "    \"今日APPS：Sago Mini Toolbox培养孩子动手能力\",\n",
    "    \"财政部副部长王保安调任国家统计局党组书记\",\n",
    "    \"2.34米男子娶1.53米女粉丝 称夫妻生活没问题\",\n",
    "    \"你看过穆赫兰道吗\",\n",
    "    \"乐视超级手机能否承载贾布斯的生态梦\"\n",
    "]\n",
    "\n",
    "for sentence in sentence_array:\n",
    "    term_list = segment.seg(sentence)\n",
    "    print(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主副食品 [0:4]\n",
      "主副食 [0:3]\n",
      "副食品 [1:4]\n",
      "副食 [1:3]\n",
      "食品 [2:4]\n",
      "最细颗粒度切分：\n",
      "主副食品 [0:4]\n",
      "主副食 [0:3]\n",
      "主 [0:1]\n",
      "副食品 [1:4]\n",
      "副食 [1:3]\n",
      "副 [1:2]\n",
      "食品 [2:4]\n",
      "食 [2:3]\n",
      "品 [3:4]\n"
     ]
    }
   ],
   "source": [
    "from jpype import *\n",
    "\n",
    "# 索引分词\n",
    "Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "IndexTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.IndexTokenizer\")\n",
    "\n",
    "term_list = IndexTokenizer.segment(\"主副食品\")\n",
    "for term in term_list.iterator():\n",
    "    print(\"{} [{}:{}]\".format(term, term.offset, term.offset + len(term.word)))\n",
    "\n",
    "print(\"最细颗粒度切分：\")\n",
    "IndexTokenizer.SEGMENT.enableIndexMode(JInt(1))  # JInt用于区分重载\n",
    "term_list = IndexTokenizer.segment(\"主副食品\")\n",
    "for term in term_list.iterator():\n",
    "    print(\"{} [{}:{}]\".format(term, term.offset, term.offset + len(term.word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP分词，更精准的中文分词、词性标注与命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[我, 新, 造, 一个, 词, 叫, 幻想乡, 你, 能, 识别, 并, 正确, 标注, 词性, 吗, ？]\n",
      "我/代词 的/助词 希望/名词 是/动词 希望/动词 张晚霞/人名 的/助词 背影/名词 被/介词 晚霞/名词 映红/人名\n",
      "支援/v 臺灣/ns 正體/n 香港/ns 繁體/n ：/v [微软/nt 公司/n]/nt 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/nr 創立/v 。/w\n"
     ]
    }
   ],
   "source": [
    "# NLP分词，更精准的中文分词、词性标注与命名实体识别\n",
    "# 标注集请查阅 https://github.com/hankcs/HanLP/blob/master/data/dictionary/other/TagPKU98.csv\n",
    "# 或者干脆调用 Sentence#translateLabels() 转为中文\n",
    "NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "print(NLPTokenizer.segment(\"我新造一个词叫幻想乡你能识别并正确标注词性吗？\"))  # “正确”是副形词。\n",
    "# 注意观察下面两个“希望”的词性、两个“晚霞”的词性\n",
    "print(NLPTokenizer.analyze(\"我的希望是希望张晚霞的背影被晚霞映红\").translateLabels())\n",
    "print(NLPTokenizer.analyze(\"支援臺灣正體香港繁體：微软公司於1975年由比爾·蓋茲和保羅·艾倫創立。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N最短路径分词，该分词器比最短路分词器慢，但是效果稍微好一些，对命名实体识别能力更强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-最短分词：[今天, ，, 刘志军, 案, 的, 关键, 人物, ,, 山西, 女, 商人, 丁书苗, 在, 市二中院, 出庭, 受审, 。] \n",
      "最短路分词：[今天, ，, 刘志军, 案, 的, 关键, 人物, ,, 山西, 女, 商人, 丁书苗, 在, 市, 二中院, 出庭, 受审, 。]\n",
      "N-最短分词：[江西省监狱管理局, 与, 中国, 太平洋, 财产保险股份有限公司南昌中心支公司, 保险, 合同, 纠纷案] \n",
      "最短路分词：[江西省监狱管理局, 与, 中国, 太平洋, 财产保险股份有限公司南昌中心支公司, 保险, 合同, 纠纷案]\n",
      "N-最短分词：[新北商贸有限公司] \n",
      "最短路分词：[新北商贸有限公司]\n"
     ]
    }
   ],
   "source": [
    "# N最短路径分词，该分词器比最短路分词器慢，但是效果稍微好一些，对命名实体识别能力更强\n",
    "sentences = [\n",
    "    \"今天，刘志军案的关键人物,山西女商人丁书苗在市二中院出庭受审。\",\n",
    "    \"江西省监狱管理局与中国太平洋财产保险股份有限公司南昌中心支公司保险合同纠纷案\",\n",
    "    \"新北商贸有限公司\",\n",
    "]\n",
    "NShortSegment = JClass(\"com.hankcs.hanlp.seg.NShort.NShortSegment\")\n",
    "Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "ViterbiSegment = JClass(\"com.hankcs.hanlp.seg.Viterbi.ViterbiSegment\")\n",
    "\n",
    "nshort_segment = NShortSegment().enableCustomDictionary(False).enablePlaceRecognize(\n",
    "    True).enableOrganizationRecognize(True)\n",
    "shortest_segment = ViterbiSegment().enableCustomDictionary(\n",
    "    False).enablePlaceRecognize(True).enableOrganizationRecognize(True)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"N-最短分词：{} \\n最短路分词：{}\".format(\n",
    "        nshort_segment.seg(sentence), shortest_segment.seg(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于AhoCorasickDoubleArrayTrie的分词器，该分词器允许用户跳过核心词典，直接使用自己的词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[微观经济学, 继续教育, 循环经济]\n"
     ]
    }
   ],
   "source": [
    "# 基于AhoCorasickDoubleArrayTrie的分词器，该分词器允许用户跳过核心词典，直接使用自己的词典。\n",
    "# 需要注意的是，自己的词典必须遵守HanLP词典格式。\n",
    "AhoCorasickDoubleArrayTrieSegment = JClass(\n",
    "    \"com.hankcs.hanlp.seg.Other.AhoCorasickDoubleArrayTrieSegment\")\n",
    "Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "\n",
    "# AhoCorasickDoubleArrayTrieSegment要求用户必须提供自己的词典路径\n",
    "segment = AhoCorasickDoubleArrayTrieSegment().loadDictionary(Config.CustomDictionaryPath[0])\n",
    "print(segment.seg(\"微观经济学继续教育循环经济\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示用户词典的动态增删"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[攻城, 狮, 逆袭, 单身, 狗, ，, 迎娶, 白富美, ，, 走上, 人生, 巅峰]\n",
      "[攻城狮, 逆袭, 单身狗, ，, 迎娶, 白富美, ，, 走上, 人生, 巅峰]\n"
     ]
    }
   ],
   "source": [
    "# 演示用户词典的动态增删\n",
    "text = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"  # 怎么可能噗哈哈！\n",
    "print(HanLP.segment(text))\n",
    "\n",
    "CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "CustomDictionary.add(\"攻城狮\")  # 动态增加\n",
    "CustomDictionary.insert(\"白富美\", \"nz 1024\")  # 强行插入\n",
    "# CustomDictionary.remove(\"攻城狮\"); # 删除词语（注释掉试试）\n",
    "CustomDictionary.add(\"单身狗\", \"nz 1024 n 1\")\n",
    "# print(CustomDictionary.get(\"单身狗\"))\n",
    "\n",
    "print(HanLP.segment(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示自定义词性,以及往词典中插入自定义词性的词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "None\n",
      "电脑品牌\n",
      "[苹果电脑, 可以, 运行, 开源, 阿尔法, 狗, 代码, 吗]\n",
      "找到了 [电脑品牌] : 苹果电脑\n",
      "\n",
      "[苹果电脑, 可以, 运行, 开源, 阿尔法狗, 代码, 吗]\n"
     ]
    }
   ],
   "source": [
    "# 演示自定义词性,以及往词典中插入自定义词性的词语\n",
    "# !!!由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!!!\n",
    "Nature = JClass(\"com.hankcs.hanlp.corpus.tag.Nature\")\n",
    "pc_nature = Nature.fromString(\"n\")\n",
    "print(pc_nature)\n",
    "# 此时系统中没有\"电脑品牌\"这个词性\n",
    "pc_nature = Nature.fromString(\"电脑品牌\")\n",
    "print(pc_nature)\n",
    "# 我们可以动态添加一个\n",
    "pc_nature = Nature.create(\"电脑品牌\")\n",
    "print(pc_nature)\n",
    "# 可以将它赋予到某个词语\n",
    "LexiconUtility = JClass(\"com.hankcs.hanlp.utility.LexiconUtility\")\n",
    "LexiconUtility.setAttribute(\"苹果电脑\", pc_nature)\n",
    "# 或者\n",
    "LexiconUtility.setAttribute(\"苹果电脑\", \"电脑品牌 1000\")\n",
    "# 它们将在分词结果中生效\n",
    "term_list = HanLP.segment(\"苹果电脑可以运行开源阿尔法狗代码吗\")\n",
    "print(term_list)\n",
    "for term in term_list:\n",
    "    if term.nature == pc_nature:\n",
    "        print(\"找到了 [{}] : {}\\n\".format(pc_nature, term.word))\n",
    "\n",
    "# 还可以直接插入到用户词典\n",
    "CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "CustomDictionary.insert(\"阿尔法狗\", \"科技名词 1024\")\n",
    "StandardTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.StandardTokenizer\")\n",
    "StandardTokenizer.SEGMENT.enablePartOfSpeechTagging(True)  # 依然支持隐马词性标注\n",
    "term_list = HanLP.segment(\"苹果电脑可以运行开源阿尔法狗代码吗\")\n",
    "print(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示正规化字符配置项的效果（繁体->简体，全角->半角，大写->小写）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[爱听4g]\n",
      "[爱听4g]\n",
      "[爱听4g]\n",
      "[爱听4g]\n",
      "[爱听4g]\n"
     ]
    }
   ],
   "source": [
    "# 演示正规化字符配置项的效果（繁体->简体，全角->半角，大写->小写）。\n",
    "# 该配置项位于hanlp.properties中，通过Normalization=true来开启\n",
    "# 切换配置后必须删除CustomDictionary.txt.bin缓存，否则只影响动态插入的新词。\n",
    "CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "\n",
    "Config.Normalization = True\n",
    "CustomDictionary.insert(\"爱听4G\", \"nz 1000\")\n",
    "print(HanLP.segment(\"爱听4g\"))\n",
    "print(HanLP.segment(\"爱听4G\"))\n",
    "print(HanLP.segment(\"爱听４G\"))\n",
    "print(HanLP.segment(\"爱听４Ｇ\"))\n",
    "print(HanLP.segment(\"愛聽４Ｇ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在import pyhanlp之前编译自己的Java class，并放入pyhanlp/static中\n",
    "import os\n",
    "from pyhanlp.static import STATIC_ROOT, HANLP_JAR_PATH\n",
    "\n",
    "java_code_path = os.path.join(STATIC_ROOT, 'MyFilter.java')\n",
    "with open(java_code_path, 'w') as out:\n",
    "    java_code = \"\"\"\n",
    "import com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary;\n",
    "import com.hankcs.hanlp.dictionary.stopword.Filter;\n",
    "import com.hankcs.hanlp.seg.common.Term;\n",
    "\n",
    "public class MyFilter implements Filter\n",
    "{\n",
    "    public boolean shouldInclude(Term term)\n",
    "    {\n",
    "        if (term.nature.startsWith('m')) return true; // 数词保留\n",
    "        return !CoreStopWordDictionary.contains(term.word); // 停用词过滤\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "    out.write(java_code)\n",
    "os.system('javac -cp {} {} -d {}'.format(HANLP_JAR_PATH, java_code_path, STATIC_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[小区, 反对, 喂养, 流浪猫, 赞成, 喂养, 小宝贝]\n",
      "[小区, 居民, 反对, 喂养, 流浪猫, 居民, 赞成, 喂养, 小宝贝]\n",
      "[小区, 居民, 有, 的, 反对, 喂养, 流浪猫, ,, 而, 有的, 居民, 却, 赞成, 喂养, 这些, 小宝贝]\n",
      "[小区, 居民, 反对, 喂养, 流浪猫, 居民, 赞成, 喂养, 小宝贝]\n",
      "[数字, 123, 保留]\n"
     ]
    }
   ],
   "source": [
    "# 编译结束才可以启动hanlp\n",
    "from pyhanlp import *\n",
    "\n",
    "CoreStopWordDictionary = JClass(\"com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary\")\n",
    "Filter = JClass(\"com.hankcs.hanlp.dictionary.stopword.Filter\")\n",
    "Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "BasicTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.BasicTokenizer\")\n",
    "NotionalTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NotionalTokenizer\")\n",
    "\n",
    "text = \"小区居民有的反对喂养流浪猫，而有的居民却赞成喂养这些小宝贝\"\n",
    "# 可以动态修改停用词词典\n",
    "CoreStopWordDictionary.add(\"居民\")\n",
    "print(NotionalTokenizer.segment(text))\n",
    "CoreStopWordDictionary.remove(\"居民\")\n",
    "print(NotionalTokenizer.segment(text))\n",
    "\n",
    "# 可以对任意分词器的结果执行过滤\n",
    "term_list = BasicTokenizer.segment(text)\n",
    "print(term_list)\n",
    "CoreStopWordDictionary.apply(term_list)\n",
    "print(term_list)\n",
    "\n",
    "# 还可以自定义过滤逻辑\n",
    "MyFilter = JClass('MyFilter')\n",
    "CoreStopWordDictionary.FILTER = MyFilter()\n",
    "print(NotionalTokenizer.segment(\"数字123的保留\"))  # “的”位于stopwords.txt所以被过滤，数字得到保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
