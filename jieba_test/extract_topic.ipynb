{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topic = 10\n",
    "n_top_words = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Jacen\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read news_data\\*.txt\n",
      "read file: news_data\\a.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.106 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file: news_data\\b.txt\n",
      "read file: news_data\\c.txt\n",
      "read done.\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "pattern = os.path.join(\"news_data\", \"*.txt\")\n",
    "print(\"read \" + pattern)\n",
    "for f_name in glob.glob(pattern):\n",
    "    with open(f_name) as f:\n",
    "        print(\"read file:\", f_name)\n",
    "        for line in f:  # one line as a document\n",
    "            words = \" \".join(jieba.cut(line))\n",
    "            docs.append(words)\n",
    "random.shuffle(docs)\n",
    "print(\"read done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform\n",
      "(64, 885)\n"
     ]
    }
   ],
   "source": [
    "print(\"transform\")\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(docs)\n",
    "tfidf = TfidfTransformer().fit_transform(counts)\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "done in 2.101s.\n",
      "Topic #0:\n",
      "扣除 分析 认为 附加 专项 细则 明确 基本 标准 额度 几项 不会 加总 猜测 2000 左右 由于 围绕 诸多 抵扣 超过 费用 5000 可能 细节\n",
      "\n",
      "Topic #1:\n",
      "工作 进一步 服务 防范 化解 重点 深化 金融风险 保险业 银行业 资金 会议 银行 保监会 实体 改革 经济 继续 指出 做好 近期 各项 加强 增强 质效\n",
      "\n",
      "Topic #2:\n",
      "建议 起征点 动态 调整 税率 常委 委员 法律 提高 扣除 提出 人大 建立 专项 问题 比较 综合 费用 优化 审稿 意见 通过 附加 依然 调整机制\n",
      "\n",
      "Topic #3:\n",
      "责任编辑 谢海平 王栋 李锋 本报记者 周潇 报道 北京 回购 消息面 应该 实现 西部 今日 政策 抓紧 开展 投放 无逆 到期 当日 央行 交易日 回笼 连续\n",
      "\n",
      "Topic #4:\n",
      "支出 扣除 住房 利息 专项 赡养 附加 政策 住房贷款 租金 子女教育 公平 定额 老人 项目 房价 教育 新增 委员 全面 进行 抚养 此次 大病 医疗\n",
      "\n",
      "Topic #5:\n",
      "专题会议 风险 金融 解读 市场 八大 委开 来看 聚焦 p2p 资本 部署 主持 刘鹤 此前 委防 观点 研究 化解 连续 反弹 短期 近期 据此 登载\n",
      "\n",
      "Topic #6:\n",
      "板块 热点 天津 网络安全 行业 跌多涨少 数字 自贸区 领涨 中国 盘面 跌幅 水泥 榜前列 两市 01 22 市场 杀跌 冲高后 创指 低迷 创指报 匮乏 午间\n",
      "\n",
      "Topic #7:\n",
      "互联网 金融风险 处置 防控 监管部门 整治 自觉 发挥 框架 职责 表示 保险 会议 银行 防范 化解 推进 得到 有力 套利 金融体系 拆解 循环 影子 抓住\n",
      "\n",
      "Topic #8:\n",
      "审议 分组 草案 常委 人大 个人所得税法 修正案 全国人大常委会 第二次 29 委员 之后 22 修正 如何 审稿 二审 起点 个人 原以为 一种 拥护 提到 上次 本人\n",
      "\n",
      "Topic #9:\n",
      "房地产 差别化 信贷政策 泡沫化 遏制 坚决 完善 进一步 防范 处置 重点 风险 分类 不良贷款 牢牢 发生 问责 政府 债务 积极 稳妥 精神 系统性 加大 认真落实\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "print(\"training...\")\n",
    "\n",
    "nmf = decomposition.NMF(n_components=n_topic).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "\n",
    "# Inverse the vectorizer vocabulary to be able\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
